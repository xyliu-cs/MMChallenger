{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration, InstructBlipConfig, AutoModelForVision2Seq\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if CUDA (GPU) is available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Load the model configuration.\n",
    "config = InstructBlipConfig.from_pretrained(\"/120040051/instructblip-vicuna-7b\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the given configuration.\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForVision2Seq.from_config(config)\n",
    "    model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Infer device map based on the available resources.\n",
    "device_map = infer_auto_device_map(model, max_memory={0: \"25GiB\", 1: \"25GiB\"}, no_split_module_classes=['InstructBlipEncoderLayer', 'InstructBlipQFormerLayer', 'LlamaDecoderLayer'])\n",
    "device_map['language_model.lm_head'] = device_map['language_projection'] = device_map[('language_model.model.embed_tokens')]\n",
    "# device_map['llm_model.model.embed_tokens'] = device_map['llm_model.lm_head'] = device_map['llm_proj']\n",
    "\n",
    "# device_map\n",
    "# offload = \"\"\n",
    "# # Load the processor and model for image processing.\n",
    "processor = InstructBlipProcessor.from_pretrained(\"/120040051/instructblip-vicuna-7b\", device_map=\"auto\")\n",
    "model = InstructBlipForConditionalGeneration.from_pretrained(\"/120040051/instructblip-vicuna-7b\",\n",
    "                                                             device_map=device_map,\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InstructBlipForConditionalGeneration.from_pretrained(\"/120040051/instructblip-vicuna-7b\")\n",
    "processor = InstructBlipProcessor.from_pretrained(\"/120040051/instructblip-vicuna-7b\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1_path = f\"/120040051/test_resource/images/verb_0308/q1_1.webp\"\n",
    "raw_image = Image.open(image_1_path).convert(\"RGB\")\n",
    "# choice_question_1 = \"What is the dog doing in the refrigerator?\\nA. mothering her kittens\\nB. servicing tables\\nC. seating customers\\nD. washing its paws\\nE. farming his land\"\n",
    "\n",
    "choice_question_1 = \"What is the dog doing in the refrigerator? (A) mothering her kittens (B) servicing tables (C) seating customers (D) washing its paws (E) farming his land\"\n",
    "\n",
    "inputs = processor(raw_image, choice_question_1, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(**inputs, max_length=50, min_length=4, do_sample=True, num_beams=5, temperature=0.2, repetition_penalty=1.5, length_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
