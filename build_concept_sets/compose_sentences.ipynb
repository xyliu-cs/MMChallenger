{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../raw_results/omcs/filtered/subjects_filtered.txt\", 'r') as file1:\n",
    "    subjs = [line.split(':')[0] for line in file1.readlines()]\n",
    "\n",
    "subjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../raw_results/omcs/filtered/gerund_phrases_filtered.txt\", 'r') as file2:\n",
    "    vps = [line.strip() for line in file2.readlines()]\n",
    "\n",
    "vps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../raw_results/omcs/filtered/locations_filtered.txt\", 'r') as file3:\n",
    "    locs = [line.split(':')[0] for line in file3.readlines()]\n",
    "\n",
    "locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lemminflect import getLemma, getInflection\n",
    "\n",
    "def check_if_singular(subject_noun):\n",
    "    words = subject_noun.split()\n",
    "    key_article = words[0]\n",
    "    key_noun = words[-1]\n",
    "    if key_article == 'a': # high priority\n",
    "        return True\n",
    "    \n",
    "    singular_set = set()\n",
    "    lemma_tup = getLemma(key_noun, upos='NOUN')\n",
    "    for lemma in lemma_tup:\n",
    "        lemma_singular_tup = getInflection(lemma, 'NN', False)\n",
    "        for lemma_singular in lemma_singular_tup:\n",
    "            singular_set.add(lemma_singular)\n",
    "\n",
    "    return key_noun in singular_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_singular(\"parents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tmp_compositional_sents.json\", 'w') as out:\n",
    "    sent_list = []\n",
    "    i = 0\n",
    "    for sub in subjs:\n",
    "        be_verb = 'is' if check_if_singular(sub) else 'are'\n",
    "        for loc in locs:\n",
    "            for vp in vps:\n",
    "                local = {}\n",
    "                # who + doing what + at where\n",
    "                local[\"id\"] = i\n",
    "                local[\"subj\"] = sub\n",
    "                local['be'] = be_verb\n",
    "                local[\"vp\"] = vp\n",
    "                local[\"loc\"] = loc\n",
    "                local[\"text\"] = sub + ' ' + be_verb + ' ' + vp + ' ' + loc # e.g. [the policemen] [are] [drinking from a bottle] [in the refrigerator] \n",
    "                # print(local[\"text\"])\n",
    "                sent_list.append(local)\n",
    "                i += 1\n",
    "                if (i % 1000 == 0):\n",
    "                    print(f\"Processed {i} sentences\")\n",
    "    print(f\"Total: Processed {i} sentences\")\n",
    "    j_f = {'data': sent_list}\n",
    "    json.dump(j_f, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
